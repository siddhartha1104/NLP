{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a term that describes breaking a document or body of text into small units called tokens. You can define tokens by certain character sequences, punctuation, or other definitions depending on the type of tokenization. Doing so makes it easier for a machine to process the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''My Name is Siddhartha Pathak. I'm in my final year of BTech Degree, highly intrested in Data Science and Machine Learning.  Thankyou !!''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "from Paragraphs --> sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') \n",
    " \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My Name is Siddhartha Pathak.',\n",
       " \"I'm in my final year of BTech Degree, highly intrested in Data Science and Machine Learning.\",\n",
       " 'Thankyou !',\n",
       " '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Name is Siddhartha Pathak.\n",
      "I'm in my final year of BTech Degree, highly intrested in Data Science and Machine Learning.\n",
      "Thankyou !\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for sen in documents:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. from paragraph --> words\n",
    "2. From Sentence --> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'Name',\n",
       " 'is',\n",
       " 'Siddhartha',\n",
       " 'Pathak',\n",
       " '.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'my',\n",
       " 'final',\n",
       " 'year',\n",
       " 'of',\n",
       " 'BTech',\n",
       " 'Degree',\n",
       " ',',\n",
       " 'highly',\n",
       " 'intrested',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " '.',\n",
       " 'Thankyou',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Name', 'is', 'Siddhartha', 'Pathak', '.']\n",
      "['I', \"'m\", 'in', 'my', 'final', 'year', 'of', 'BTech', 'Degree', ',', 'highly', 'intrested', 'in', 'Data', 'Science', 'and', 'Machine', 'Learning', '.']\n",
      "['Thankyou', '!']\n",
      "['!']\n"
     ]
    }
   ],
   "source": [
    "for sentences in documents:\n",
    "    print(word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'Name',\n",
       " 'is',\n",
       " 'Siddhartha',\n",
       " 'Pathak',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'in',\n",
       " 'my',\n",
       " 'final',\n",
       " 'year',\n",
       " 'of',\n",
       " 'BTech',\n",
       " 'Degree',\n",
       " ',',\n",
       " 'highly',\n",
       " 'intrested',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " '.',\n",
       " 'Thankyou',\n",
       " '!!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'Name',\n",
       " 'is',\n",
       " 'Siddhartha',\n",
       " 'Pathak.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'my',\n",
       " 'final',\n",
       " 'year',\n",
       " 'of',\n",
       " 'BTech',\n",
       " 'Degree',\n",
       " ',',\n",
       " 'highly',\n",
       " 'intrested',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Machine',\n",
       " 'Learning.',\n",
       " 'Thankyou',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "# fullstop will not be treated as a seperate word but at last fullstop will be treated as a seperate word\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
